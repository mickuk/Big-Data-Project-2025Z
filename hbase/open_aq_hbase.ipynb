{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "396b4971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Pobieranie danych z Hive...\n",
      "2. Czyszczenie danych...\n",
      "   -> Filtruję dane od 2025-12-01...\n",
      "3. Agregacja (Pivot)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Powstałe kolumny:\n",
      "root\n",
      " |-- location_name: string (nullable = true)\n",
      " |-- data_pomiaru: string (nullable = true)\n",
      " |-- pm1: double (nullable = true)\n",
      " |-- pm25: double (nullable = true)\n",
      " |-- relativehumidity: double (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- um003: double (nullable = true)\n",
      " |-- processed_at: timestamp (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Liczba wierszy (Dzień x Stacja) do wstawienia: 195\n",
      "4. Resetowanie tabeli HBase...\n",
      "Tabela 'air_quality_metrics' już istnieje. Usuwam starą wersję...\n",
      "Tworzę nową, czystą tabelę HBase: air_quality_metrics\n",
      "5. Zapisywanie do HBase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 52:=====================================================>(197 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- SUKCES! Załadowano dane od 1 grudnia 2025 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import happybase\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp, avg, date_format, current_timestamp, round as spark_round\n",
    "\n",
    "# --- 1. KONFIGURACJA ---\n",
    "HBASE_HOST = 'localhost'\n",
    "HBASE_TABLE = 'air_quality_metrics'\n",
    "HIVE_RAW_TABLE = 'air_quality'\n",
    "\n",
    "# --- 2. FUNKCJE HBASE ---\n",
    "def recreate_table():\n",
    "    \"\"\"Usuwa tabelę jeśli istnieje i tworzy nową (Clean Slate).\"\"\"\n",
    "    try:\n",
    "        conn = happybase.Connection(HBASE_HOST)\n",
    "        tables = conn.tables()\n",
    "        \n",
    "        if HBASE_TABLE.encode() in tables:\n",
    "            print(f\"Tabela '{HBASE_TABLE}' już istnieje. Usuwam starą wersję...\")\n",
    "            if conn.is_table_enabled(HBASE_TABLE):\n",
    "                conn.disable_table(HBASE_TABLE)\n",
    "            conn.delete_table(HBASE_TABLE)\n",
    "        \n",
    "        print(f\"Tworzę nową, czystą tabelę HBase: {HBASE_TABLE}\")\n",
    "        conn.create_table(HBASE_TABLE, {'stats': dict(), 'meta': dict()})\n",
    "        conn.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Błąd krytyczny przy tworzeniu tabeli: {e}\")\n",
    "\n",
    "def save_partition_to_hbase(iterator):\n",
    "    try:\n",
    "        conn = happybase.Connection(HBASE_HOST)\n",
    "        table = conn.table(HBASE_TABLE)\n",
    "        batch = table.batch()\n",
    "        \n",
    "        for row in iterator:\n",
    "            row_key = f\"{row.location_name}#{row.data_pomiaru}\"\n",
    "            \n",
    "            data = {\n",
    "                b'meta:location': str(row.location_name).encode('utf-8'),\n",
    "                b'meta:date': str(row.data_pomiaru).encode('utf-8'),\n",
    "                b'meta:processed_at': str(row.processed_at).encode('utf-8')\n",
    "            }\n",
    "            \n",
    "            row_dict = row.asDict()\n",
    "            meta_cols = ['location_name', 'data_pomiaru', 'processed_at']\n",
    "            \n",
    "            for col_name, value in row_dict.items():\n",
    "                if col_name not in meta_cols and value is not None:\n",
    "                    hb_col = f'stats:{col_name}'.encode('utf-8')\n",
    "                    hb_val = str(value).encode('utf-8')\n",
    "                    data[hb_col] = hb_val\n",
    "            \n",
    "            batch.put(row_key, data)\n",
    "        \n",
    "        batch.send()\n",
    "        conn.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Błąd zapisu w partycji: {e}\")\n",
    "\n",
    "# --- 3. LOGIKA SPARKA ---\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AirQuality_Full_Pivot_Reload\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"1. Pobieranie danych z Hive...\")\n",
    "df_raw = spark.table(HIVE_RAW_TABLE)\n",
    "\n",
    "print(\"2. Czyszczenie danych...\")\n",
    "# A. Parsowanie daty\n",
    "df_parsed = df_raw.withColumn(\"ts_parsed\", to_timestamp(col(\"datetimelocal\"), \"dd-MM-yyyy HH:mm:ss\"))\n",
    "\n",
    "# B. Filtrowanie: Tylko od 1 GRUDNIA 2025\n",
    "print(\"   -> Filtruję dane od 2025-12-01...\")\n",
    "df_filtered = df_parsed.filter(col(\"ts_parsed\").isNotNull()) \\\n",
    "                       .filter(col(\"ts_parsed\") >= \"2025-12-01\") # <--- ZMIANA DATY\n",
    "\n",
    "# C. Usuwanie duplikatów (Stacja + Czas + Parametr)\n",
    "df_cleaned = df_filtered.dropDuplicates(['location_name', 'datetimelocal', 'parameter'])\n",
    "\n",
    "# Formatowanie daty do grupowania (YYYY-MM-DD)\n",
    "df_final = df_cleaned.withColumn(\"data_pomiaru\", date_format(col(\"ts_parsed\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "print(\"3. Agregacja (Pivot)...\")\n",
    "pivot_df = df_final.groupBy(\"location_name\", \"data_pomiaru\") \\\n",
    "    .pivot(\"parameter\") \\\n",
    "    .agg(spark_round(avg(\"value\"), 2)) \\\n",
    "    .withColumn(\"processed_at\", current_timestamp())\n",
    "\n",
    "print(\"   Powstałe kolumny:\")\n",
    "pivot_df.printSchema()\n",
    "print(f\"   Liczba wierszy (Dzień x Stacja) do wstawienia: {pivot_df.count()}\")\n",
    "\n",
    "print(\"4. Resetowanie tabeli HBase...\")\n",
    "recreate_table()\n",
    "\n",
    "print(\"5. Zapisywanie do HBase...\")\n",
    "pivot_df.foreachPartition(save_partition_to_hbase)\n",
    "\n",
    "print(\"--- SUKCES! Załadowano dane od 1 grudnia 2025 ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b8d2bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pobieram 5 przykładowych wierszy z HBase:\n",
      "Klucz: Belgradzka#2025-12-01\n",
      "   Dane: {b'meta:date': b'2025-12-01', b'meta:location': b'Belgradzka', b'meta:processed_at': b'2026-01-09 23:10:52.732000', b'stats:pm1': b'25.6', b'stats:pm25': b'46.7', b'stats:relativehumidity': b'60.3', b'stats:temperature': b'9.56', b'stats:um003': b'4880.0'}\n",
      "Klucz: Belgradzka#2025-12-02\n",
      "   Dane: {b'meta:date': b'2025-12-02', b'meta:location': b'Belgradzka', b'meta:processed_at': b'2026-01-09 23:10:52.732000', b'stats:pm1': b'18.8', b'stats:pm25': b'34.9', b'stats:relativehumidity': b'62.8', b'stats:temperature': b'8.91', b'stats:um003': b'3230.0'}\n",
      "Klucz: Belgradzka#2025-12-03\n",
      "   Dane: {b'meta:date': b'2025-12-03', b'meta:location': b'Belgradzka', b'meta:processed_at': b'2026-01-09 23:10:52.732000', b'stats:pm1': b'17.4', b'stats:pm25': b'33.0', b'stats:relativehumidity': b'61.1', b'stats:temperature': b'10.0', b'stats:um003': b'3030.0'}\n",
      "Klucz: Belgradzka#2025-12-04\n",
      "   Dane: {b'meta:date': b'2025-12-04', b'meta:location': b'Belgradzka', b'meta:processed_at': b'2026-01-09 23:10:52.732000', b'stats:pm1': b'22.1', b'stats:pm25': b'40.2', b'stats:relativehumidity': b'60.5', b'stats:temperature': b'10.1', b'stats:um003': b'4220.0'}\n",
      "Klucz: Belgradzka#2025-12-05\n",
      "   Dane: {b'meta:date': b'2025-12-05', b'meta:location': b'Belgradzka', b'meta:processed_at': b'2026-01-09 23:10:52.732000', b'stats:pm1': b'22.1', b'stats:pm25': b'39.7', b'stats:relativehumidity': b'65.2', b'stats:temperature': b'11.0', b'stats:um003': b'4060.0'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/09 23:13:58 WARN util.JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/blockmgr-914d484d-16c7-4dca-8b1c-5a53da5373ec. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /tmp/blockmgr-914d484d-16c7-4dca-8b1c-5a53da5373ec\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:171)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:110)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1141)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:182)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:178)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:178)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.stop(DiskBlockManager.scala:173)\n",
      "\tat org.apache.spark.storage.BlockManager.stop(BlockManager.scala:1931)\n",
      "\tat org.apache.spark.SparkEnv.stop(SparkEnv.scala:92)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$23(SparkContext.scala:2108)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1419)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2108)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$37(SparkContext.scala:661)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)\n"
     ]
    }
   ],
   "source": [
    "# Szybki test odczytu\n",
    "conn = happybase.Connection('localhost')\n",
    "table = conn.table('air_quality_metrics')\n",
    "\n",
    "print(\"Pobieram 5 przykładowych wierszy z HBase:\")\n",
    "for key, data in table.scan(limit=5):\n",
    "    print(f\"Klucz: {key.decode()}\")\n",
    "    print(f\"   Dane: {data}\")\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
